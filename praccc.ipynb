{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxaTjytnSAZYjlYyh2JH9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MALLIKARJUNABATHALA999/DEEPLEARNING/blob/main/praccc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InadIKGZTQJU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "# Network and training parameters.\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10   # number of outputs = number of digits\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
        "# Loading MNIST dataset.\n",
        "# verify\n",
        "# You can verify that the split between train and test is 60,000, and 10,000 respectively.\n",
        "# Labels have one-hot representation.is automatically applied\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "# X_train is 60000 rows of 28x28 values; we  --> reshape it to 60000 x 784.\n",
        "RESHAPED = 784\n",
        "#\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "# Normalize inputs to be within in [0, 1].\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "print(X_train.shape[1], 'image dimesnions')\n",
        "print(X_test.shape[1], 'image pixels')\n",
        "# One-hot representation of the labels.\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model.\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(NB_CLASSES,\n",
        "            input_shape=(RESHAPED,),\n",
        "            name='dense_layer',\n",
        "            activation='softmax'))\n"
      ],
      "metadata": {
        "id": "RyUD3sXnTWet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model.\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "DTvo3mNvTZnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model.\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n"
      ],
      "metadata": {
        "id": "vkEq6xxgThFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "v2C677AXTjQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Network and training.\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10   # number of outputs = number of digits\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
        "# Loading MNIST dataset.\n",
        "# Labels have one-hot representation.\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
        "RESHAPED = 784\n",
        "#\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "# Normalize inputs to be within in [0, 1].\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "# Labels have one-hot representation.\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "# Build the model.\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "             input_shape=(RESHAPED,),\n",
        "             name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "             name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dense(NB_CLASSES,\n",
        "             name='dense_layer_3', activation='softmax'))\n",
        "# Summary of the model.\n",
        "model.summary()\n",
        "# Compiling the model.\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Training the model.\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "# Evaluating the model.\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "KvVQc2wlTzZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "# Network and training.\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10   # number of outputs = number of digits\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
        "DROPOUT = 0.3\n",
        "# Loading MNIST dataset.\n",
        "# Labels have one-hot representation.\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
        "RESHAPED = 784\n",
        "#\n",
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "# Normalize inputs within [0, 1].\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "# One-hot representations for labels.\n",
        "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
        "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
        "# Building the model.\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "              input_shape=(RESHAPED,),\n",
        "              name='dense_layer', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(N_HIDDEN,\n",
        "              name='dense_layer_2', activation='relu'))\n",
        "model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.Dense(NB_CLASSES,\n",
        "              name='dense_layer_3', activation='softmax'))\n",
        "# Summary of the model.\n",
        "model.summary()\n",
        "# Compiling the model.\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Training the model.\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
        "# Evaluating the model.\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "not training for long enough\n",
        "ReLU is not differentiable at 0. We can however extend the first derivative at 0 to a function over the whole domain by defining it to be either a 0 or 1.\n",
        "\n",
        "The piecewise derivative of ReLU"
      ],
      "metadata": {
        "id": "QXJ7M08eT2Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model.\n",
        "model.compile(optimizer='RMSProp',\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "O6vDskF2T--n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "import tensorflow_datasets as tfds\n",
        "max_len = 200\n",
        "n_words = 10000\n",
        "dim_embedding = 256\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 500\n",
        "def load_data():\n",
        "    # Load data.\n",
        "    (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n",
        "    # Pad sequences with max_len.\n",
        "    X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
        "    X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
        "    return (X_train, y_train), (X_test, y_test)\n"
      ],
      "metadata": {
        "id": "msakY2aryUe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    # Input: - eEmbedding Layer.\n",
        "    # The model will take as input an integer matrix of size (batch, input_length).\n",
        "    # The model will output dimension (input_length, dim_embedding).\n",
        "    # The largest integer in the input should be no larger\n",
        "    # than n_words (vocabulary size).\n",
        "    model.add(layers.Embedding(n_words,\n",
        "        dim_embedding, input_length=max_len))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    # Takes the maximum value of either feature vector from each of the n_words features.\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "E9zxRGJvyU_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = load_data()\n",
        "model = build_model()\n",
        "model.summary()\n",
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")\n",
        "score = model.fit(X_train, y_train,\n",
        " epochs = EPOCHS,\n",
        " batch_size = BATCH_SIZE,\n",
        " validation_data = (X_test, y_test)\n",
        ")\n",
        "score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
        "print(\"\\nTest score:\", score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "metadata": {
        "id": "syAp4phry__h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions.\n",
        "predictions = model.predict(X)\n"
      ],
      "metadata": {
        "id": "Df8p3Ld7zs7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n"
      ],
      "metadata": {
        "id": "kp3cR38ezu9p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, optimizers\n",
        "# network and training\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "OPTIMIZER = tf.keras.optimizers.Adam()\n",
        "VALIDATION_SPLIT=0.90\n",
        "IMG_ROWS, IMG_COLS = 28, 28 # input image dimensions\n",
        "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
        "NB_CLASSES = 10  # number of outputs = number of digits\n"
      ],
      "metadata": {
        "id": "Qai79KbBQAtO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build(input_shape, classes):\n",
        "    model = models.Sequential()\n",
        "    # CONV => RELU => POOL\n",
        "    model.add(layers.Convolution2D(20, (5, 5), activation='relu',\n",
        "            input_shape=INPUT_SHAPE))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# CONV => RELU => POOL\n",
        "    model.add(layers.Convolution2D(50, (5, 5), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    # Flatten => RELU layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(500, activation='relu'))\n",
        "# a softmax classifier\n",
        "    model.add(layers.Dense(classes, activation=\"softmax\"))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "sfezbk_NQljO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data: shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n",
        "# reshape\n",
        "X_train = X_train.reshape((60000, 28, 28, 1))\n",
        "X_test = X_test.reshape((10000, 28, 28, 1))\n",
        "# normalize\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "# cast\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)\n",
        "# initialize the optimizer and model\n",
        "model = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n",
        "    metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "# use TensorBoard, princess Aurora!\n",
        "callbacks = [\n",
        "  # Write TensorBoard logs to './logs' directory\n",
        "  tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
        "]\n",
        "# fit\n",
        "history = model.fit(X_train, y_train,\n",
        "        batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "        verbose=VERBOSE, validation_split=VALIDATION_SPLIT,\n",
        "        callbacks=callbacks)\n",
        "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\n",
        "print(\"\\nTest score:\", score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "metadata": {
        "id": "mOvni0UjQBVq",
        "outputId": "4db24e1b-164b-49f3-9ddd-47eb3693742d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LeNet' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bdc25e7bef15>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# initialize the optimizer and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNB_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n\u001b[1;32m     17\u001b[0m     metrics=[\"accuracy\"])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LeNet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONV => RELU => POOL\n",
        "model.add(layers.Convolution2D(50, (5, 5), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n"
      ],
      "metadata": {
        "id": "v7GwPAGmQFo1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten => RELU layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(500, activation='relu'))\n",
        "# a softmax classifier\n",
        "model.add(layers.Dense(NB_CLASSES, activation=\"softmax\"))\n",
        "return model\n"
      ],
      "metadata": {
        "id": "tNuCforhQF-A",
        "outputId": "b736c1f0-03ec-4970-a472-91e2860c2057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'return' outside function (<ipython-input-13-afa477f05bd8>, line 6)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-afa477f05bd8>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    return model\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZ4LXShUQU9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}